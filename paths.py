


yangjie_rich_pretrain_unigram_path = '../data/word/gigaword_chn.all.a2b.uni.ite50.vec'# 0-11326 'é“¹ -0.437016 0.343134 -0.772380 0.615054 0.009950 -0.231971 -0.405644 -0.221314 0.616823 0.001931 0.043979 -0.315271 0.126433 -0.277143 -0.309668 -0.361442 0.259386 -0.746785 -0.198396 -1.706697 0.498408 0.227801 0.068913 -0.506231 0.083333 -0.319355 -0.424380 -0.040148 0.159290 -0.559228 -0.381768 0.583252 -0.108047 0.524855 -1.314263 1.002900 -0.769172 -1.229506 -0.360265 0.020058 -0.895712 0.273854 0.238782 0.413841 0.555397 0.143341 -0.407146 0.324613 0.939674 0.768922 \n'

yangjie_rich_pretrain_bigram_path = '../data/word/gigaword_chn.all.a2b.bi.ite50.vec'
yangjie_rich_pretrain_word_path = '../data/word/ctb.50d.vec' # embedding_path  yangjie's rich pretrain word list: 704368*50[n*d]
yangjie_rich_pretrain_char_and_word_path = '../data/word/yangjie_word_char_mix.txt'
# lk_word_path = '/remote-home/xnli/data/pretrain/chinese/sgns.merge.word'
lk_word_path_2 = '../data/word/sgns.merge.word'



ontonote4ner_cn_path = '../data/datasets/OntoNote4NER'
msra_ner_cn_path = '../data/datasets/msra'
resume_ner_path = '../data/datasets/ResumeNER'
weibo_ner_path = '../data/datasets/Weibo'



# yangjie_rich_pretrain_unigram_path = '{}/gigaword_chn.all.a2b.uni.ite50.vec'
# yangjie_rich_pretrain_bigram_path = '{}/gigaword_chn.all.a2b.bi.ite50.vec'
# yangjie_rich_pretrain_word_path = '{}/ctb.50d.vec'
#
# # this path is for the output of preprocessing
# yangjie_rich_pretrain_char_and_word_path = '{}/yangjie_word_char_mix.txt'
#
#
#
# ontonote4ner_cn_path = '{}/OntoNote4NER'
# msra_ner_cn_path = '{}/MSRANER'
# resume_ner_path = '{}/ResumeNER'
# weibo_ner_path = '{}/WeiboNER'